{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8548099c-e167-42ff-acfd-e7e2ffa7ab98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "for i in [\n",
    "    \"stock_calendar\",\n",
    "    \"stock_listing\",\n",
    "    \"stock_detailed_listing\",\n",
    "    \"stock_detailed_reviews\",\n",
    "    \"berlin_calendar\",\n",
    "    \"berlin_listing\",\n",
    "    \"berlin_detailed_listing\",\n",
    "    \"berlin_detailed_reviews\",\n",
    "]:\n",
    "    print(i)\n",
    "    df = spark.table(\n",
    "        \"some_catalog.airbnb_data.\" + i\n",
    "    )\n",
    "    # Detect and remove rows where all columns are null\n",
    "    all_null_condition = reduce(\n",
    "        lambda a, b: a & b,\n",
    "        [F.col(c).isNull() for c in df.columns]\n",
    "    )\n",
    "    all_null_count = df.filter(\n",
    "        all_null_condition\n",
    "    ).count()\n",
    "    df = df.filter(~all_null_condition)\n",
    "    total_rows = df.count()\n",
    "    summary = []\n",
    "    for c in df.columns:\n",
    "        nan_count = df.filter(\n",
    "            F.col(c).isNull()\n",
    "        ).count()\n",
    "        percent_missing = (\n",
    "            (nan_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        )\n",
    "        summary.append(\n",
    "            (c, total_rows, nan_count, percent_missing)\n",
    "        )\n",
    "    summary_df = spark.createDataFrame(\n",
    "        summary,\n",
    "        [\"column\", \"total_rows\", \"missing_count\", \"missing_percent\"]\n",
    "    )\n",
    "    print(f\"Rows with all columns null: {all_null_count}\")\n",
    "    display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63a67b2-79e6-4b17-a1e1-fb9a54eb3b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"some_catalog.airbnb_bronze.berlin_listing\")\n",
    "distribution_df = (\n",
    "    df.groupBy(\"number_of_reviews\")\n",
    "    .count()\n",
    "    .orderBy(\"number_of_reviews\")\n",
    ")\n",
    "display(distribution_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-10 16_48_17",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
